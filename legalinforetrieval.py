# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dbXLuvOTMgfkkzJJs3HHmoCMxEALPZFw
"""
#Have to implement tf idf for matched words>word2vec
#preprocessing query
def retrieve(query):
    import nltk
    import re

   
    from nltk.stem.snowball import SnowballStemmer
    from nltk.stem.wordnet import WordNetLemmatizer
    from nltk.corpus import stopwords
    from string import punctuation
    from nltk.corpus import words
    
    snow_stemmer = SnowballStemmer(language='english')
    wnl = WordNetLemmatizer()

    query=query.lower()
    tokens=[]

    #for tf-idf
    tokenstf=[]
    for word in query.split(' '):
        #if word not in stopwords.words('english'):
        for char in set(punctuation).union(set(['\n','\t','\r','(',')','[',']'])):
              word=word.replace(char,' ')
        tokenstf.append(wnl.lemmatize(word))

    


    for word in query.split(' '):
      tokens.append(word)
    import pandas
    import numpy as np
    dataframe=pandas.read_csv('./SIH-Processed.csv');
    if query=='':
        dataframe['score']=np.zeros(len(dataframe))
        return dataframe
    #tf of query
    vocabtf=list(set(tokenstf))
    tf_query=np.zeros(len(vocabtf))
    for i in range(len(vocabtf)):
        for token in tokenstf:
            if token==vocabtf[i]:
                tf_query[i]+=1
                
    #removing stop words and punctuation
    tokens_nopunct=[]
    for token in tokens:
      if token not in stopwords.words('english') and token not in punctuation:
        tokens_nopunct.append(wnl.lemmatize(token))

    lemma_list=tokens_nopunct
                
    #searching tf in judgments
    filenames=dataframe['filename']
    tf=np.zeros((len(filenames),len(vocabtf)))
    idf=np.zeros(len(vocabtf))
    tf_idf=np.zeros((len(filenames),len(vocabtf)))
    for i in range(len(filenames)):
        with open("judgment_refined/"+filenames[i]+".txt",'r',errors="ignore") as f:
            lines=f.readlines()
        filestring=' '.join(lines)
        
        for j in range(len(vocabtf)):
            tf[i][j]+=len(re.findall(r'\s'+vocabtf[j]+'\s',filestring))
            idf[j]+=len(re.findall(r'\s'+vocabtf[j]+'\s',filestring))

    if idf.all()==0:
        file1=open('vocab_spelling.txt','r',encoding='utf-8')
        correct_words=file1.read().split(',')
        file1.close()
        from nltk.metrics.distance  import edit_distance

        incorrect_words=lemma_list
          
        # loop for finding correct spellings
        # based on edit distance and
        # printing the correct words
        for word in incorrect_words:
            temp = [(edit_distance(word, w),w) for w in correct_words]
            return 'Sorry! Nothing found! Did you mean? \n'+str([w[1] for w in sorted(temp, key = lambda val:val[0])[:5]])



    tf_idf=tf/idf

    cosinelisttf=[]
    for ti in tf_idf:
        cosinelisttf.append(sum(tf_query*ti))
        
    

    

    queries=dataframe['case name']
    q=[]
    y=[]
    x=lemma_list
    for qu in queries:
      y.append(qu.lower())


    freq=dataframe['frequency of querying']
    from scipy.spatial.distance import cosine

    tf_idf={}
    for word in x:
        for query in queries:
            if query not in tf_idf:
                tf_idf.setdefault(query,0)
            tf_idf[query]+=len(re.findall(word,query.lower()))
            

    

    dataframe['score']=list(tf_idf.values())
    dataframe['tfscore-query']=list(tf_idf.values())
    dataframe['tf-idf-score']=np.array(cosinelisttf)
    dataframe['score']+=(np.array(cosinelisttf)*20)
    #training word2vec model and embedding query
    ##urls=['https://indiankanoon.org/doc/603097/','https://www.advocatekhoj.com/library/judgments/announcement.php?WID=14838','https://www.advocatekhoj.com/library/judgments/announcement.php?WID=14837','https://www.advocatekhoj.com/library/judgments/announcement.php?WID=14835','https://www.advocatekhoj.com/library/judgments/announcement.php?WID=14834','https://www.advocatekhoj.com/library/judgments/index.php?go=1983/july/1.php',
    ##      'https://www.advocatekhoj.com/library/judgments/index.php?go=2021/august/4.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2021/august/12.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2020/december/2.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2021/march/24.php','https://www.advocatekhoj.com/library/judgments/announcement.php?WID=14831',
    ##      'https://www.advocatekhoj.com/library/judgments/index.php?go=2006/january/1.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2006/december/3.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2006/november/1.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2006/january/1.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2021/january/11.php',
    ##      'https://www.advocatekhoj.com/library/judgments/index.php?go=1983/january/1.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2006/february/5.php','https://www.advocatekhoj.com/library/judgments/index.php?go=1992/september/1.php','https://www.advocatekhoj.com/library/judgments/index.php?go=1992/january/1.php','https://www.advocatekhoj.com/library/judgments/index.php?go=2021/february/51.php',
    ##      'https://www.casemine.com/judgement/in/5609af33e4b0149711415cab','https://www.livelaw.in/top-stories/supreme-court-holds-a-major-daughter-not-entitled-to-education-expenses-from-father-as-she-does-not-want-to-maintain-relationship-with-him-194363','https://www.advocatekhoj.com/library/judgments/index.php?go=1969/january/7.php']

    import bs4 as bs
    import re
    from gensim.models import Word2Vec
    import requests
    import gensim.downloader as api
    import gensim


    ##article_text=""
    ##for url in urls:
    ##    print(url)
    ##page = requests.get(url)
    ##ml_wiki = page.text
    ##ml_wiki_parsed = bs.BeautifulSoup(ml_wiki,'lxml')
    ##paragraphs = ml_wiki_parsed.find_all('p')
    ##for p in paragraphs:
    ##    para_text = p.text.lower()
    ##    #cleaning the text
    ##    #para_text = re.sub('[^a-zA-Z]', ' ', para_text)
    ##    #para_text=re.sub(r'\s+', ' ', para_text)
    ##    article_text+=para_text

    ##
    ##all_words=[]
    ##i=0
    ##from nltk.corpus import stopwords
    ##for filename in ['computertrain','booktrain']:
    ##  with open(filename+".txt",'r',errors="ignore") as f:
    ##    lines=f.readlines()
    ##  article_text=''
    ##  for words in lines:
    ##    article_text+=words.lower()
    ##  all_sentences = nltk.sent_tokenize(article_text)
    ##  
    ##  for sent in all_sentences:
    ##    wordsinsent=[]
    ##    for word in sent.split(' '):
    ##      if word not in stopwords.words('english') and word in keywords[i]:
    ##        for char in set(punctuation).union(set(['\n','\t','\r','(',')','[',']'])):
    ##          word=word.replace(char,'')
    ##        wordsinsent.append(snow_stemmer.stem(word))
    ##    all_words.append(wordsinsent)
    ##  i+=1

    #print(all_words)
    #word2vec = Word2Vec(all_words, min_count=2)
    word2vec = Word2Vec.load("word2vec.model")
    query_embedding = [word2vec.wv[w] for w in lemma_list if w in word2vec.wv.key_to_index]

    #similarity matching
    from scipy.spatial.distance import cosine
    cosinelist=[]
    if query_embedding!=[]:
        query_word2vec=sum(query_embedding)/len(query_embedding)
        for w in dataframe['word2vec_embedding']:
          w=str(w)
          w=w.replace(']','')
          w=w.replace('[','')
          w=np.array(list(w.split()),dtype=np.float32)
          cosinelist.append(1-cosine(w,query_word2vec))
    else:
        cosinelist=np.zeros((len(dataframe.index)))
##        #print(w,'and',w.shape,'and',query_word2vec.shape,'and',type(w),type(query_word2vec))
##    print(cosinelist)

    #using google word2vec
    #path = api.load("word2vec-google-news-300", return_path=True)
    #print(path)
    #wvgoogle=gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)
    #wvgoogle=gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Toshiba/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', binary=True)
    #query_embedding_google = [wvgoogle[w] for w in lemma_list if w in wvgoogle.key_to_index]
    #cosinelist_google=[]
    #if query_embedding_google!=[]:
    #    query_word2vec_google=sum(query_embedding_google)/len(query_embedding_google)
    #    for w in dataframe['word2vec_embedding_google']:
    #      w=str(w)
    #      w=w.replace(']','')
    #      w=w.replace('[','')
    #      w=np.array(list(w.split()),dtype=np.float32)
    #      cosinelist_google.append(1-cosine(w,query_word2vec_google))
    #else:
    #    cosinelist_google=np.zeros((len(dataframe.index)))
      
    ##keyvocab=set(tokens_nopunct)
    ##
    ##for filename in ['computertrain','booktrain']:
    ##  with open(filename+".txt",'r',errors="ignore") as f:
    ##    lines=f.readlines()
    ##  tf_idf_query=np.zeros((1,len(keyvocab))
    ##keyvocab=list(keyvocab)
    ##for i in range(len(keyvocab)):
    ##  if keyvocab[i] in tokens_nopunct:
    ##    tf_idf_query[i]=1
    ##                      
    ##tf_idf_abstracts=[]
    ##for abstract in abstracts:
    ##  tf_idf_abstract=np.zeros((1,len(keyvocab))
    ##  for i in range(len(keyvocab)):
    ##    if keyvocab[i] in abstract:
    ##      tf_idf_abstract[i]=1
    ##  tf_idf_abstracts.append(tf_idf_abstract)
    ##
    ##  
    ##                      
        

    dataframe['word2vecscore']=np.array(cosinelist)
    #dataframe['word2vecgooglescore']=np.array(cosinelist_google)
    score=np.array(cosinelist)*5
    #score=np.array(cosinelist_google)*3
    dataframe['score']*=2
    dataframe['score']+=score

    


    #Filter out judgments with score<threshold(0.5)
    filtered_df = dataframe[dataframe['score'] >= 0]

    #Apply heuristics
    year=np.array(list(filtered_df['case year']),dtype=np.int32);
    citation_score=np.array(list(filtered_df['citation_score(random number to illustrate the working of alg)']),dtype=np.int32);
    court_score=np.array(list(filtered_df['court_score(random number to illustrate the working of the alg)']),dtype=np.int32);
    score=np.array(filtered_df['score'],dtype=np.float32)

    filtered_df['score']=(score+(0.3*citation_score)+(0.2*court_score))+(0.0001*year)
    filtered_df['heuristicscore']=(score+(0.3*citation_score)+(0.2*court_score))+(0.0001*year)
    #Constructing the adjacency matrix
    adj_matrix=pandas.DataFrame()
    for jno in filtered_df['case number']:
      adj_matrix[jno]=np.zeros(len(filtered_df['case number']))
      
    adj_matrix['i']=list(filtered_df['case number'])
    
    adj_matrix.set_index('i',inplace=True)

    for ind in filtered_df.index:
      cn=filtered_df['case number'][ind]
      rstring=filtered_df['judgements_referred(cites)'][ind]
      rl1=rstring.split(',')
      
      rl=[int(j.strip()) for j in rl1 if int(j.strip()) in list(filtered_df['case number'])]
      
      for j in rl:
        adj_matrix[j][cn]=1/len(rl)
      
    print("adj:\n",adj_matrix)
    #calculating probability of reaching each node
    probability_of_reaching_node={}
    pagerankscorenew={}
    cs=filtered_df['citation_score(random number to illustrate the working of alg)']
    totalcs=sum(cs)
    
    for ind in filtered_df.index:
        index=filtered_df['case number'][ind]
        csvalue=float(filtered_df['citation_score(random number to illustrate the working of alg)'][ind])
        probability_of_reaching_node.setdefault(index,csvalue/totalcs)
        
        pagerankscorenew.setdefault(index,0)

    
    index=0
    for i in probability_of_reaching_node:
       
        index+=1

    pagerankscore=probability_of_reaching_node
    
    #page rank
    diff=1
    while diff!=0: #check for convergence
      diff=0
      for j in probability_of_reaching_node:
        adj=np.array(adj_matrix[j],dtype=np.float32)
        probs=np.array(list(pagerankscore.values()),dtype=np.float32)
        pagerankscorenew[j]=sum(adj*probs)+float(probability_of_reaching_node[j])
        diff+=abs(pagerankscorenew[j]-pagerankscore[j])
      pagerankscore=pagerankscorenew
      
    score=np.array(filtered_df['score'],dtype=np.float32)
    filtered_df['pagerankscore']=np.array(list(pagerankscore.values()))
    filtered_df['score']=score+(np.array(list(pagerankscore.values()),dtype=np.float32)*0.2)
    pandas.set_option('display.max_columns', None)
    return filtered_df.sort_values(by=['score'],ascending=False)
    #Apply filters

#print(retrieve(input("Enter query")))
